{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\vinay\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vinay\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\vinay\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize,TreebankWordTokenizer\n",
    "from nltk.stem import PorterStemmer,SnowballStemmer,WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = '''Natural Language Processing (NLP) is a field of artificial intelligence, focused on the interaction between computers and human language. \n",
    "It allows machines to understand, interpret, and generate text in a meaningful way. \n",
    "Common applications include sentiment analysis, language translation, and chatbots. \n",
    "NLP uses techniques like tokenization, named entity recognition, and part-of-speech tagging. \n",
    "The goal is to bridge the gap between human communication and machine understanding, making interactions smoother and more efficient.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Tokenization\n",
    "* Paragraph to Sentences using Sent Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Natural Language Processing (NLP) is a field of artificial intelligence, focused on the interaction between computers and human language.',\n",
       " 'It allows machines to understand, interpret, and generate text in a meaningful way.',\n",
       " 'Common applications include sentiment analysis, language translation, and chatbots.',\n",
       " 'NLP uses techniques like tokenization, named entity recognition, and part-of-speech tagging.',\n",
       " 'The goal is to bridge the gap between human communication and machine understanding, making interactions smoother and more efficient.']"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences=sent_tokenize(paragraph)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural Language Processing (NLP) is a field of artificial intelligence, focused on the interaction between computers and human language.\n",
      "It allows machines to understand, interpret, and generate text in a meaningful way.\n",
      "Common applications include sentiment analysis, language translation, and chatbots.\n",
      "NLP uses techniques like tokenization, named entity recognition, and part-of-speech tagging.\n",
      "The goal is to bridge the gap between human communication and machine understanding, making interactions smoother and more efficient.\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "    print (sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Tokenization\n",
    "* Paragraph to words using word tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Natural',\n",
       " 'Language',\n",
       " 'Processing',\n",
       " '(',\n",
       " 'NLP',\n",
       " ')',\n",
       " 'is',\n",
       " 'a',\n",
       " 'field',\n",
       " 'of',\n",
       " 'artificial',\n",
       " 'intelligence',\n",
       " ',',\n",
       " 'focused',\n",
       " 'on',\n",
       " 'the',\n",
       " 'interaction',\n",
       " 'between',\n",
       " 'computers',\n",
       " 'and',\n",
       " 'human',\n",
       " 'language',\n",
       " '.',\n",
       " 'It',\n",
       " 'allows',\n",
       " 'machines',\n",
       " 'to',\n",
       " 'understand',\n",
       " ',',\n",
       " 'interpret',\n",
       " ',',\n",
       " 'and',\n",
       " 'generate',\n",
       " 'text',\n",
       " 'in',\n",
       " 'a',\n",
       " 'meaningful',\n",
       " 'way',\n",
       " '.',\n",
       " 'Common',\n",
       " 'applications',\n",
       " 'include',\n",
       " 'sentiment',\n",
       " 'analysis',\n",
       " ',',\n",
       " 'language',\n",
       " 'translation',\n",
       " ',',\n",
       " 'and',\n",
       " 'chatbots',\n",
       " '.',\n",
       " 'NLP',\n",
       " 'uses',\n",
       " 'techniques',\n",
       " 'like',\n",
       " 'tokenization',\n",
       " ',',\n",
       " 'named',\n",
       " 'entity',\n",
       " 'recognition',\n",
       " ',',\n",
       " 'and',\n",
       " 'part-of-speech',\n",
       " 'tagging',\n",
       " '.',\n",
       " 'The',\n",
       " 'goal',\n",
       " 'is',\n",
       " 'to',\n",
       " 'bridge',\n",
       " 'the',\n",
       " 'gap',\n",
       " 'between',\n",
       " 'human',\n",
       " 'communication',\n",
       " 'and',\n",
       " 'machine',\n",
       " 'understanding',\n",
       " ',',\n",
       " 'making',\n",
       " 'interactions',\n",
       " 'smoother',\n",
       " 'and',\n",
       " 'more',\n",
       " 'efficient',\n",
       " '.']"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words=word_tokenize(paragraph)\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Sentence to words using word tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words=[]\n",
    "# for i in range(len(sentences)):\n",
    "#     word=word_tokenize(sentences[i])\n",
    "#     words.append(word)\n",
    "# words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Natural'"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Treebank Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Natural',\n",
       " 'Language',\n",
       " 'Processing',\n",
       " '(',\n",
       " 'NLP',\n",
       " ')',\n",
       " 'is',\n",
       " 'a',\n",
       " 'field',\n",
       " 'of',\n",
       " 'artificial',\n",
       " 'intelligence',\n",
       " ',',\n",
       " 'focused',\n",
       " 'on',\n",
       " 'the',\n",
       " 'interaction',\n",
       " 'between',\n",
       " 'computers',\n",
       " 'and',\n",
       " 'human',\n",
       " 'language.',\n",
       " 'It',\n",
       " 'allows',\n",
       " 'machines',\n",
       " 'to',\n",
       " 'understand',\n",
       " ',',\n",
       " 'interpret',\n",
       " ',',\n",
       " 'and',\n",
       " 'generate',\n",
       " 'text',\n",
       " 'in',\n",
       " 'a',\n",
       " 'meaningful',\n",
       " 'way.',\n",
       " 'Common',\n",
       " 'applications',\n",
       " 'include',\n",
       " 'sentiment',\n",
       " 'analysis',\n",
       " ',',\n",
       " 'language',\n",
       " 'translation',\n",
       " ',',\n",
       " 'and',\n",
       " 'chatbots.',\n",
       " 'NLP',\n",
       " 'uses',\n",
       " 'techniques',\n",
       " 'like',\n",
       " 'tokenization',\n",
       " ',',\n",
       " 'named',\n",
       " 'entity',\n",
       " 'recognition',\n",
       " ',',\n",
       " 'and',\n",
       " 'part-of-speech',\n",
       " 'tagging.',\n",
       " 'The',\n",
       " 'goal',\n",
       " 'is',\n",
       " 'to',\n",
       " 'bridge',\n",
       " 'the',\n",
       " 'gap',\n",
       " 'between',\n",
       " 'human',\n",
       " 'communication',\n",
       " 'and',\n",
       " 'machine',\n",
       " 'understanding',\n",
       " ',',\n",
       " 'making',\n",
       " 'interactions',\n",
       " 'smoother',\n",
       " 'and',\n",
       " 'more',\n",
       " 'efficient',\n",
       " '.']"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer=TreebankWordTokenizer()\n",
    "tokenizer.tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'natural language processing (nlp) is a field of artificial intelligence, focused on the interaction between computers and human language. \\nit allows machines to understand, interpret, and generate text in a meaningful way. \\ncommon applications include sentiment analysis, language translation, and chatbots. \\nnlp uses techniques like tokenization, named entity recognition, and part-of-speech tagging. \\nthe goal is to bridge the gap between human communication and machine understanding, making interactions smoother and more efficient.'"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer=PorterStemmer()\n",
    "stemmer.stem(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural------>natur\n",
      "Language------>languag\n",
      "Processing------>process\n",
      "(------>(\n",
      "NLP------>nlp\n",
      ")------>)\n",
      "is------>is\n",
      "a------>a\n",
      "field------>field\n",
      "of------>of\n",
      "artificial------>artifici\n",
      "intelligence------>intellig\n",
      ",------>,\n",
      "focused------>focus\n",
      "on------>on\n",
      "the------>the\n",
      "interaction------>interact\n",
      "between------>between\n",
      "computers------>comput\n",
      "and------>and\n",
      "human------>human\n",
      "language------>languag\n",
      ".------>.\n",
      "It------>it\n",
      "allows------>allow\n",
      "machines------>machin\n",
      "to------>to\n",
      "understand------>understand\n",
      ",------>,\n",
      "interpret------>interpret\n",
      ",------>,\n",
      "and------>and\n",
      "generate------>gener\n",
      "text------>text\n",
      "in------>in\n",
      "a------>a\n",
      "meaningful------>meaning\n",
      "way------>way\n",
      ".------>.\n",
      "Common------>common\n",
      "applications------>applic\n",
      "include------>includ\n",
      "sentiment------>sentiment\n",
      "analysis------>analysi\n",
      ",------>,\n",
      "language------>languag\n",
      "translation------>translat\n",
      ",------>,\n",
      "and------>and\n",
      "chatbots------>chatbot\n",
      ".------>.\n",
      "NLP------>nlp\n",
      "uses------>use\n",
      "techniques------>techniqu\n",
      "like------>like\n",
      "tokenization------>token\n",
      ",------>,\n",
      "named------>name\n",
      "entity------>entiti\n",
      "recognition------>recognit\n",
      ",------>,\n",
      "and------>and\n",
      "part-of-speech------>part-of-speech\n",
      "tagging------>tag\n",
      ".------>.\n",
      "The------>the\n",
      "goal------>goal\n",
      "is------>is\n",
      "to------>to\n",
      "bridge------>bridg\n",
      "the------>the\n",
      "gap------>gap\n",
      "between------>between\n",
      "human------>human\n",
      "communication------>commun\n",
      "and------>and\n",
      "machine------>machin\n",
      "understanding------>understand\n",
      ",------>,\n",
      "making------>make\n",
      "interactions------>interact\n",
      "smoother------>smoother\n",
      "and------>and\n",
      "more------>more\n",
      "efficient------>effici\n",
      ".------>.\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word+\"------>\"+stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Snwball Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural---------->natur\n",
      "Language---------->languag\n",
      "Processing---------->process\n",
      "(---------->(\n",
      "NLP---------->nlp\n",
      ")---------->)\n",
      "is---------->is\n",
      "a---------->a\n",
      "field---------->field\n",
      "of---------->of\n",
      "artificial---------->artifici\n",
      "intelligence---------->intellig\n",
      ",---------->,\n",
      "focused---------->focus\n",
      "on---------->on\n",
      "the---------->the\n",
      "interaction---------->interact\n",
      "between---------->between\n",
      "computers---------->comput\n",
      "and---------->and\n",
      "human---------->human\n",
      "language---------->languag\n",
      ".---------->.\n",
      "It---------->it\n",
      "allows---------->allow\n",
      "machines---------->machin\n",
      "to---------->to\n",
      "understand---------->understand\n",
      ",---------->,\n",
      "interpret---------->interpret\n",
      ",---------->,\n",
      "and---------->and\n",
      "generate---------->generat\n",
      "text---------->text\n",
      "in---------->in\n",
      "a---------->a\n",
      "meaningful---------->meaning\n",
      "way---------->way\n",
      ".---------->.\n",
      "Common---------->common\n",
      "applications---------->applic\n",
      "include---------->includ\n",
      "sentiment---------->sentiment\n",
      "analysis---------->analysi\n",
      ",---------->,\n",
      "language---------->languag\n",
      "translation---------->translat\n",
      ",---------->,\n",
      "and---------->and\n",
      "chatbots---------->chatbot\n",
      ".---------->.\n",
      "NLP---------->nlp\n",
      "uses---------->use\n",
      "techniques---------->techniqu\n",
      "like---------->like\n",
      "tokenization---------->token\n",
      ",---------->,\n",
      "named---------->name\n",
      "entity---------->entiti\n",
      "recognition---------->recognit\n",
      ",---------->,\n",
      "and---------->and\n",
      "part-of-speech---------->part-of-speech\n",
      "tagging---------->tag\n",
      ".---------->.\n",
      "The---------->the\n",
      "goal---------->goal\n",
      "is---------->is\n",
      "to---------->to\n",
      "bridge---------->bridg\n",
      "the---------->the\n",
      "gap---------->gap\n",
      "between---------->between\n",
      "human---------->human\n",
      "communication---------->communic\n",
      "and---------->and\n",
      "machine---------->machin\n",
      "understanding---------->understand\n",
      ",---------->,\n",
      "making---------->make\n",
      "interactions---------->interact\n",
      "smoother---------->smoother\n",
      "and---------->and\n",
      "more---------->more\n",
      "efficient---------->effici\n",
      ".---------->.\n"
     ]
    }
   ],
   "source": [
    "stemmer=SnowballStemmer('english')\n",
    "for word in words:\n",
    "    print(word+\"---------->\"+stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural----------->Natural\n",
      "Language----------->Language\n",
      "Processing----------->Processing\n",
      "(----------->(\n",
      "NLP----------->NLP\n",
      ")----------->)\n",
      "is----------->is\n",
      "a----------->a\n",
      "field----------->field\n",
      "of----------->of\n",
      "artificial----------->artificial\n",
      "intelligence----------->intelligence\n",
      ",----------->,\n",
      "focused----------->focused\n",
      "on----------->on\n",
      "the----------->the\n",
      "interaction----------->interaction\n",
      "between----------->between\n",
      "computers----------->computer\n",
      "and----------->and\n",
      "human----------->human\n",
      "language----------->language\n",
      ".----------->.\n",
      "It----------->It\n",
      "allows----------->allows\n",
      "machines----------->machine\n",
      "to----------->to\n",
      "understand----------->understand\n",
      ",----------->,\n",
      "interpret----------->interpret\n",
      ",----------->,\n",
      "and----------->and\n",
      "generate----------->generate\n",
      "text----------->text\n",
      "in----------->in\n",
      "a----------->a\n",
      "meaningful----------->meaningful\n",
      "way----------->way\n",
      ".----------->.\n",
      "Common----------->Common\n",
      "applications----------->application\n",
      "include----------->include\n",
      "sentiment----------->sentiment\n",
      "analysis----------->analysis\n",
      ",----------->,\n",
      "language----------->language\n",
      "translation----------->translation\n",
      ",----------->,\n",
      "and----------->and\n",
      "chatbots----------->chatbots\n",
      ".----------->.\n",
      "NLP----------->NLP\n",
      "uses----------->us\n",
      "techniques----------->technique\n",
      "like----------->like\n",
      "tokenization----------->tokenization\n",
      ",----------->,\n",
      "named----------->named\n",
      "entity----------->entity\n",
      "recognition----------->recognition\n",
      ",----------->,\n",
      "and----------->and\n",
      "part-of-speech----------->part-of-speech\n",
      "tagging----------->tagging\n",
      ".----------->.\n",
      "The----------->The\n",
      "goal----------->goal\n",
      "is----------->is\n",
      "to----------->to\n",
      "bridge----------->bridge\n",
      "the----------->the\n",
      "gap----------->gap\n",
      "between----------->between\n",
      "human----------->human\n",
      "communication----------->communication\n",
      "and----------->and\n",
      "machine----------->machine\n",
      "understanding----------->understanding\n",
      ",----------->,\n",
      "making----------->making\n",
      "interactions----------->interaction\n",
      "smoother----------->smoother\n",
      "and----------->and\n",
      "more----------->more\n",
      "efficient----------->efficient\n",
      ".----------->.\n"
     ]
    }
   ],
   "source": [
    "lemmatizer=WordNetLemmatizer()\n",
    "for word in words:\n",
    "    print(word+\"----------->\"+lemmatizer.lemmatize(word,pos='n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sentences)):\n",
    "    words=word_tokenize(sentences[i])\n",
    "    words=[stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i]=\" \".join(words) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['natur languag process ( nlp ) field artifici intellig , focus interact comput human languag .',\n",
       " 'it allow machin understand , interpret , generat text meaning way .',\n",
       " 'common applic includ sentiment analysi , languag translat , chatbot .',\n",
       " 'nlp use techniqu like token , name entiti recognit , part-of-speech tag .',\n",
       " 'the goal bridg gap human communic machin understand , make interact smoother effici .']"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sentences)):\n",
    "    words=word_tokenize(sentences[i])\n",
    "    words=[lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i]=\" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['natur languag process ( nlp ) field artifici intellig , focus interact comput human languag .',\n",
       " 'allow machin understand , interpret , generat text meaning way .',\n",
       " 'common applic includ sentiment analysi , languag translat , chatbot .',\n",
       " 'nlp use techniqu like token , name entiti recognit , part-of-speech tag .',\n",
       " 'goal bridg gap human communic machin understand , make interact smoother effici .']"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parts of Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('natur', 'JJ'), ('languag', 'NN'), ('process', 'NN'), ('(', '('), ('nlp', 'JJ'), (')', ')'), ('field', 'NN'), ('artifici', 'NN'), ('intellig', 'NN'), (',', ','), ('focus', 'VB'), ('interact', 'JJ'), ('comput', 'NN'), ('human', 'JJ'), ('languag', 'NN'), ('.', '.')]\n",
      "[('allow', 'VB'), ('machin', 'NN'), ('understand', 'NN'), (',', ','), ('interpret', 'NN'), (',', ','), ('generat', 'FW'), ('text', 'IN'), ('meaning', 'VBG'), ('way', 'NN'), ('.', '.')]\n",
      "[('common', 'JJ'), ('applic', 'JJ'), ('includ', 'JJ'), ('sentiment', 'NN'), ('analysi', 'NN'), (',', ','), ('languag', 'JJ'), ('translat', 'NN'), (',', ','), ('chatbot', 'NN'), ('.', '.')]\n",
      "[('nlp', 'NN'), ('use', 'NN'), ('techniqu', 'NN'), ('like', 'IN'), ('token', 'NN'), (',', ','), ('name', 'NN'), ('entiti', 'JJ'), ('recognit', 'NN'), (',', ','), ('part-of-speech', 'JJ'), ('tag', 'NN'), ('.', '.')]\n",
      "[('goal', 'NN'), ('bridg', 'NN'), ('gap', 'NN'), ('human', 'JJ'), ('communic', 'NN'), ('machin', 'NN'), ('understand', 'NN'), (',', ','), ('make', 'VBP'), ('interact', 'JJ'), ('smoother', 'JJR'), ('effici', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(sentences)):\n",
    "    words=word_tokenize(sentences[i])\n",
    "    words=[lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    #sentences[i]=\" \".join(words)\n",
    "    pos_tag=nltk.pos_tag(words)\n",
    "    print(pos_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv=CountVectorizer(max_features=15)\n",
    "vectors=cv.fit_transform(sentences).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 15)"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 2, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'natur': 6,\n",
       " 'languag': 2,\n",
       " 'process': 9,\n",
       " 'nlp': 7,\n",
       " 'interact': 1,\n",
       " 'human': 0,\n",
       " 'machin': 3,\n",
       " 'understand': 14,\n",
       " 'text': 11,\n",
       " 'meaning': 4,\n",
       " 'translat': 13,\n",
       " 'techniqu': 10,\n",
       " 'token': 12,\n",
       " 'name': 5,\n",
       " 'of': 8}"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binary Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'natur': 6,\n",
       " 'languag': 2,\n",
       " 'process': 9,\n",
       " 'nlp': 7,\n",
       " 'interact': 1,\n",
       " 'human': 0,\n",
       " 'machin': 3,\n",
       " 'understand': 14,\n",
       " 'text': 11,\n",
       " 'meaning': 4,\n",
       " 'translat': 13,\n",
       " 'techniqu': 10,\n",
       " 'token': 12,\n",
       " 'name': 5,\n",
       " 'of': 8}"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv=CountVectorizer(max_features=15,binary=True,ngram_range=(1,1))\n",
    "vectors=cv.fit_transform(sentences).toarray()\n",
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'languag': 3,\n",
       " 'process': 13,\n",
       " 'nlp': 6,\n",
       " 'interact': 2,\n",
       " 'human': 1,\n",
       " 'nlp field': 7,\n",
       " 'allow': 0,\n",
       " 'machin': 4,\n",
       " 'understand': 14,\n",
       " 'machin understand': 5,\n",
       " 'part': 11,\n",
       " 'of': 9,\n",
       " 'nlp use': 8,\n",
       " 'part of': 12,\n",
       " 'of speech': 10}"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv=CountVectorizer(max_features=15,binary=True,ngram_range=(1,2))\n",
    "vectors=cv.fit_transform(sentences).toarray()\n",
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'natur languag': 8,\n",
       " 'nlp field': 10,\n",
       " 'natur languag process': 9,\n",
       " 'nlp field artifici': 11,\n",
       " 'allow machin': 0,\n",
       " 'machin understand': 1,\n",
       " 'meaning way': 5,\n",
       " 'nlp use': 12,\n",
       " 'name entiti': 6,\n",
       " 'of speech': 14,\n",
       " 'nlp use techniqu': 13,\n",
       " 'name entiti recognit': 7,\n",
       " 'make interact': 3,\n",
       " 'machin understand make': 2,\n",
       " 'make interact smoother': 4}"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv=CountVectorizer(max_features=15,binary=True,ngram_range=(2,3))\n",
    "vectors=cv.fit_transform(sentences).toarray()\n",
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF (Term Frequency - Inverse Document Frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf=TfidfVectorizer(max_features=15)\n",
    "vectors=tfidf.fit_transform(sentences).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.31508619, 0.31508619, 0.63017237, 0.        , 0.        ,\n",
       "       0.        , 0.39054121, 0.31508619, 0.        , 0.39054121,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ])"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'natur': 6,\n",
       " 'languag': 2,\n",
       " 'process': 9,\n",
       " 'nlp': 7,\n",
       " 'interact': 1,\n",
       " 'human': 0,\n",
       " 'machin': 3,\n",
       " 'understand': 14,\n",
       " 'text': 11,\n",
       " 'meaning': 4,\n",
       " 'translat': 13,\n",
       " 'techniqu': 10,\n",
       " 'token': 12,\n",
       " 'name': 5,\n",
       " 'of': 8}"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.31508619, 0.31508619, 0.63017237, 0.        ,\n",
       "       0.        , 0.31508619, 0.39054121, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.39054121, 0.        ])"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf=TfidfVectorizer(max_features=15,ngram_range=(1,2))\n",
    "vectors=tfidf.fit_transform(sentences).toarray()\n",
    "vectors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'languag': 3,\n",
       " 'process': 13,\n",
       " 'nlp': 6,\n",
       " 'interact': 2,\n",
       " 'human': 1,\n",
       " 'nlp field': 7,\n",
       " 'allow': 0,\n",
       " 'machin': 4,\n",
       " 'understand': 14,\n",
       " 'machin understand': 5,\n",
       " 'part': 11,\n",
       " 'of': 9,\n",
       " 'nlp use': 8,\n",
       " 'part of': 12,\n",
       " 'of speech': 10}"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.5, 0.5, 0.5, 0.5, 0. ,\n",
       "       0. , 0. ])"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf=TfidfVectorizer(max_features=15,ngram_range=(2,3))\n",
    "vectors=tfidf.fit_transform(sentences).toarray()\n",
    "vectors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'natur languag': 8,\n",
       " 'nlp field': 10,\n",
       " 'natur languag process': 9,\n",
       " 'nlp field artifici': 11,\n",
       " 'allow machin': 0,\n",
       " 'machin understand': 1,\n",
       " 'meaning way': 5,\n",
       " 'nlp use': 12,\n",
       " 'name entiti': 6,\n",
       " 'of speech': 14,\n",
       " 'nlp use techniqu': 13,\n",
       " 'name entiti recognit': 7,\n",
       " 'make interact': 3,\n",
       " 'machin understand make': 2,\n",
       " 'make interact smoother': 4}"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim .models import word2vec,keyedvectors\n",
    "import gensim.downloader as api\n",
    "wv=api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gensim.models.keyedvectors.KeyedVectors"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.80859375e-01, -1.44531250e-01,  3.02734375e-01, -2.11914062e-01,\n",
       "       -1.74804688e-01,  4.24804688e-02,  2.53906250e-01, -2.04101562e-01,\n",
       "       -8.93554688e-02,  2.75878906e-02,  9.81445312e-02, -2.71484375e-01,\n",
       "       -1.62353516e-02,  1.68945312e-01,  2.55859375e-01,  2.70996094e-02,\n",
       "        6.98242188e-02,  1.34765625e-01,  7.81250000e-02,  1.95312500e-01,\n",
       "       -8.53538513e-05, -3.17382812e-02,  7.12890625e-02, -5.00000000e-01,\n",
       "        3.78906250e-01,  1.36718750e-01,  4.83398438e-02,  5.11718750e-01,\n",
       "        6.52343750e-01, -1.94335938e-01, -2.01171875e-01,  9.71679688e-02,\n",
       "       -3.55468750e-01,  1.28173828e-02,  4.80957031e-02,  7.56835938e-02,\n",
       "       -2.91015625e-01, -3.24707031e-02,  2.09960938e-02, -1.23535156e-01,\n",
       "        1.91650391e-02, -5.27343750e-02,  2.26562500e-01,  1.01562500e-01,\n",
       "       -8.15429688e-02, -3.12500000e-01,  1.46484375e-01, -3.24218750e-01,\n",
       "        4.22363281e-02,  1.91406250e-01, -1.64062500e-01,  1.96289062e-01,\n",
       "        2.26562500e-01,  2.25830078e-03,  2.92968750e-01, -4.12109375e-01,\n",
       "       -2.49023438e-01, -4.35546875e-01, -1.60156250e-01, -2.48046875e-01,\n",
       "       -2.81250000e-01, -4.17480469e-02, -3.53515625e-01,  2.87109375e-01,\n",
       "        1.80664062e-02,  1.96533203e-02, -2.44140625e-01,  2.05078125e-02,\n",
       "        3.53515625e-01, -7.08007812e-02,  2.14843750e-01, -7.76367188e-02,\n",
       "       -2.20703125e-01, -1.50390625e-01, -1.28906250e-01, -2.26562500e-01,\n",
       "        1.39648438e-01, -2.53906250e-01,  4.84375000e-01, -3.49609375e-01,\n",
       "       -2.55859375e-01,  2.67578125e-01, -1.08886719e-01,  2.30468750e-01,\n",
       "       -1.53320312e-01, -7.91015625e-02, -9.03320312e-02,  1.72851562e-01,\n",
       "       -3.20312500e-01,  1.17675781e-01,  9.47265625e-02, -6.10351562e-02,\n",
       "       -4.60815430e-03, -1.76757812e-01, -8.93554688e-02,  2.18750000e-01,\n",
       "        3.37890625e-01, -6.83593750e-02,  5.32226562e-02, -7.99560547e-03,\n",
       "       -1.04492188e-01,  4.51660156e-02,  7.91015625e-02, -9.71679688e-02,\n",
       "        2.51953125e-01, -3.51562500e-02,  3.26171875e-01, -1.28906250e-01,\n",
       "       -8.74023438e-02, -5.66406250e-02, -4.79125977e-03,  1.70898438e-01,\n",
       "        2.33398438e-01, -2.26562500e-01,  4.72656250e-01,  2.63671875e-01,\n",
       "       -5.98144531e-02, -6.44531250e-02,  3.33984375e-01,  5.32226562e-02,\n",
       "       -3.41796875e-02,  3.22265625e-02, -9.71679688e-02, -3.00292969e-02,\n",
       "       -3.45703125e-01,  4.31640625e-01, -2.42187500e-01,  3.33984375e-01,\n",
       "       -5.00488281e-02, -1.61132812e-01, -2.39562988e-03,  6.29882812e-02,\n",
       "       -2.75390625e-01, -5.23437500e-01, -2.34375000e-01, -3.36914062e-02,\n",
       "       -2.59765625e-01,  3.78906250e-01, -9.27734375e-02,  9.52148438e-02,\n",
       "        6.98242188e-02, -2.94189453e-02,  3.66210938e-02,  4.66308594e-02,\n",
       "       -3.04687500e-01,  1.88476562e-01, -2.16064453e-02, -3.28125000e-01,\n",
       "       -1.56402588e-03,  2.07031250e-01, -1.83105469e-02, -1.52343750e-01,\n",
       "       -8.30078125e-02, -2.71484375e-01, -3.39843750e-01,  6.19506836e-03,\n",
       "        3.30078125e-01, -4.68750000e-02, -4.51660156e-02, -1.01074219e-01,\n",
       "       -1.96289062e-01, -2.59765625e-01,  1.88476562e-01, -2.51953125e-01,\n",
       "       -2.40234375e-01, -5.15625000e-01,  3.39843750e-01, -2.07031250e-01,\n",
       "        2.08007812e-01, -1.07421875e-01, -5.95703125e-02,  1.61132812e-01,\n",
       "        7.08007812e-02, -1.72851562e-01,  5.17578125e-02, -8.39843750e-02,\n",
       "       -1.95312500e-01, -2.41210938e-01, -1.22680664e-02,  1.60156250e-01,\n",
       "        1.04980469e-02,  3.78906250e-01,  2.05078125e-02, -3.14453125e-01,\n",
       "        2.06054688e-01,  2.80761719e-02,  4.90722656e-02,  1.00097656e-01,\n",
       "        3.35937500e-01, -4.15039062e-02, -1.21582031e-01,  1.93359375e-01,\n",
       "       -3.45703125e-01,  1.42578125e-01, -2.43164062e-01, -2.79541016e-02,\n",
       "       -5.20019531e-02,  1.44531250e-01,  1.26953125e-01, -3.10546875e-01,\n",
       "        5.62500000e-01, -1.60156250e-01, -1.92871094e-02, -4.88281250e-02,\n",
       "       -2.67028809e-03,  1.07910156e-01,  3.11279297e-03, -4.10156250e-01,\n",
       "        2.40234375e-01, -2.33398438e-01, -5.22460938e-02,  1.21093750e-01,\n",
       "        1.74804688e-01, -2.09960938e-01,  8.59375000e-02, -2.19726562e-01,\n",
       "       -3.24218750e-01,  6.88476562e-02, -6.49414062e-02, -2.98828125e-01,\n",
       "       -1.63085938e-01, -7.42187500e-02, -4.04296875e-01,  2.38281250e-01,\n",
       "       -1.87500000e-01,  7.12890625e-02,  6.34765625e-02, -3.78906250e-01,\n",
       "        1.56250000e-01, -7.22656250e-02, -3.04687500e-01,  1.16699219e-01,\n",
       "        2.61718750e-01,  1.35742188e-01,  2.15820312e-01, -8.42285156e-03,\n",
       "        6.36718750e-01, -3.26171875e-01,  2.65625000e-01, -7.27539062e-02,\n",
       "        8.97216797e-03, -7.95898438e-02,  4.98046875e-01, -2.69531250e-01,\n",
       "        1.26953125e-01, -1.62109375e-01, -1.52343750e-01,  9.13085938e-02,\n",
       "       -1.41601562e-01,  3.75000000e-01, -1.40380859e-02,  1.66992188e-01,\n",
       "       -2.00195312e-01,  3.24218750e-01,  2.73437500e-01,  3.35937500e-01,\n",
       "        4.17480469e-02, -6.22558594e-02,  3.73535156e-02,  6.93359375e-02,\n",
       "        1.44531250e-01,  3.44238281e-02,  1.46484375e-01, -3.36914062e-02,\n",
       "       -1.03515625e-01, -1.25976562e-01, -5.23437500e-01, -8.25195312e-02,\n",
       "       -1.75781250e-01, -3.04687500e-01, -8.34960938e-02,  2.53906250e-01,\n",
       "       -9.96093750e-02,  8.05664062e-02,  2.03125000e-01, -2.02148438e-01,\n",
       "        4.86328125e-01, -2.51464844e-02, -4.95605469e-02, -1.97265625e-01,\n",
       "        5.05371094e-02,  1.22070312e-03,  2.89306641e-02,  3.85742188e-02,\n",
       "        8.39843750e-02,  1.84570312e-01,  3.78906250e-01,  1.73339844e-02,\n",
       "       -5.10253906e-02,  2.79296875e-01,  2.03125000e-01, -2.19726562e-01,\n",
       "        7.71484375e-02, -2.91015625e-01,  1.29882812e-01,  3.96728516e-03,\n",
       "        5.07812500e-02,  3.88671875e-01, -2.27539062e-01,  1.18164062e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv['Cricket']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cricket', 0.7541396617889404),\n",
       " ('Twenty##', 0.6863775253295898),\n",
       " ('Cricket_Board', 0.6725560426712036),\n",
       " ('cricketing', 0.6518699526786804),\n",
       " ('cricketers', 0.6509546041488647),\n",
       " ('Twenty##_cricket', 0.6462868452072144),\n",
       " ('Twenty/##', 0.6459011435508728),\n",
       " ('twenty##', 0.6422706842422485),\n",
       " ('Wales_Cricket', 0.6269664764404297),\n",
       " ('IPL', 0.6072497963905334)]"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar('Cricket')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
